# spec-ai Configuration (lmstudio)
# This example shows how to configure hierarchical reasoning with a fast model
# for preliminary tasks and a main model for complex reasoning

# Default agent with multi-model reasoning
default_agent = "default"

# Database configuration
[database]
path = "~/.spec-ai/demo-lmstudio_agent_data.db"

# Main model configuration (for complex reasoning)
[model]
provider = "lmstudio"
model_name = "qwen/qwen3-vl-8b"
embeddings_model = "text-embedding-embeddinggemma-300m"
temperature = 0.7

# User interface configuration
[ui]
theme = "default"
prompt="spec-ai> "

# Logging configuration
[logging]
level = "info"

# ========== MULTI-MODEL AGENT CONFIGURATION ==========
[agents.default]
# Basic agent configuration
prompt = "You're an autonomous assistant with access to tools."
temperature = 0.7
memory_k = 20
top_p = 0.9

# These will prompt approval
denied_tools = [
    "audio_transcribe",
    "bash",
    "file_extract",
    "file_read",
    "file_write",
    "search",
    "shell",
    "web_scraper",
    "web_search"
]

# Offline transcription
[audio]
provider = "vttrs"
model = "whisper-large-v3"
on_device = true


# ========== HIERARCHICAL MULTI-MODEL REASONING ==========
# Enable fast reasoning with Llama-3.2-3B
fast_reasoning = true

# Fast model provider configuration
# LM Studio for local serving, or Ollama for cross-platform deployment
fast_model_provider = "lmstudio"  # Options: "lmstudio", "ollama", "mlx"

# Llama-3.2-3B model (4-bit quantized for efficiency)
fast_model_name = "llama-3.2-3b-instruct"

# For Ollama, use:
# fast_model_provider = "ollama"
# fast_model_name = "llama3.2:3b"

# Lower temperature for consistent fast model responses
fast_model_temperature = 0.3

# Tasks delegated to the fast model
# Removed: decision_routing, tool_selection, classification (these prevent action-taking)
fast_model_tasks = [
    "entity_extraction",      # Extract names, dates, URLs, emails
    "graph_analysis",         # Analyze graph relationships
    "confidence_scoring",    # Assess response confidence
    "summarization",         # Quick summaries of short texts
    "syntax_checking",       # Check code syntax
]
style = "professional"

# Confidence threshold for escalation to main model
# If fast model confidence < threshold, escalate to main model
escalation_threshold = 0.6

# ========== KNOWLEDGE GRAPH (OPTIONAL) ==========
# Can be combined with multi-model reasoning
enable_graph = true
graph_memory = true
auto_graph = true
graph_steering = true
graph_depth = 3
graph_weight = 0.5

# ========== FAST LOCAL AGENT (Llama Only) ==========
[agents.fast_local]
prompt = "You are a fast, efficient assistant running locally."
temperature = 0.3
memory_k = 5

# Use only the fast model (no main model)
model_provider = "lmstudio"
model_name = "llama-3.2-3b-instruct"

# No hierarchical reasoning (single model)
fast_reasoning = false

# ========== RESEARCH AGENT WITH FAST PRE-PROCESSING ==========
[agents.researcher]
prompt = """You are a research assistant that uses fast models for initial
document processing and entity extraction, then uses the main model for
analysis and synthesis."""
temperature = 0.5
memory_k = 30

# Enable multi-model for research
fast_reasoning = true
fast_model_provider = "lmstudio"
fast_model_name = "llama-3.2-3b-instruct"
fast_model_temperature = 0.2

# Research-specific fast tasks
fast_model_tasks = [
    "entity_extraction",     # Extract entities from papers
    "reference_parsing",     # Parse citations
    "keyword_extraction",    # Extract key terms
    "initial_categorization", # Categorize documents
    "metadata_extraction",   # Extract doc metadata
]

escalation_threshold = 0.7  # Higher threshold for research accuracy

# ========== CODING ASSISTANT WITH SYNTAX CHECKING ==========
[agents.coder]
prompt = """You are a coding assistant that uses a fast model for syntax checking
and simple code analysis, and the main model for complex refactoring and design."""
temperature = 0.3
memory_k = 25

# Enable fast model for code tasks
fast_reasoning = true
fast_model_provider = "lmstudio"
fast_model_name = "llama-3.2-3b-instruct"
fast_model_temperature = 0.1  # Very low for deterministic syntax checking

fast_model_tasks = [
    "syntax_checking",       # Check code syntax
    "linting",              # Basic code linting
    "import_resolution",     # Resolve imports
    "variable_tracking",     # Track variable usage
    "simple_completions",    # Complete simple patterns
]

escalation_threshold = 0.5  # Escalate complex logic quickly