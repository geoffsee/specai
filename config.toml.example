# Spec-AI Configuration
# Multi-model reasoning and knowledge graph are enabled by default

# Default agent with advanced features
default_agent = "default"

# Database configuration
[database]
# Path to the DuckDB database file
path = "~/.agent_cli/agent_data.duckdb"

# Model configuration (Main model for complex reasoning)
[model]
# Model provider: "openai", "anthropic", "ollama", "mlx", or "mock"
provider = "openai"

# Model name (provider-specific, optional)
# OpenAI: "gpt-4", "gpt-3.5-turbo"
# Anthropic: "claude-3-opus-20240229", "claude-3-sonnet-20240229"
# Ollama: "llama3", "mistral"
# model_name = "gpt-4"

# Embeddings model for semantic search
embeddings_model = "text-embedding-ada-002"

# API key source (optional)
# Examples: "env:OPENAI_API_KEY", "file:~/.secrets/api_key"
# api_key_source = "env:OPENAI_API_KEY"

# Temperature for main model
temperature = 0.7

# UI configuration
[ui]
# Theme: "default", "dark", "light"
theme = "default"
# Prompt string
prompt = "> "

# Logging configuration
[logging]
# Log level: "trace", "debug", "info", "warn", "error"
level = "info"

# ========== DEFAULT AGENT WITH ALL FEATURES ==========
[agents.default]
# System prompt
prompt = """You are an intelligent assistant with advanced reasoning capabilities.
You use hierarchical multi-model reasoning for optimal performance and maintain
a knowledge graph to track context and relationships across conversations."""

# Temperature for main model responses
temperature = 0.7

# Memory configuration
memory_k = 20  # Number of messages to recall
top_p = 0.9    # Top-p sampling for memory recall

# ========== KNOWLEDGE GRAPH (DEFAULT: ENABLED) ==========
enable_graph = true         # Build and use knowledge graph
graph_memory = true         # Use graph for memory recall
auto_graph = true          # Automatically extract entities and relationships
graph_steering = true      # Let graph influence decisions
graph_depth = 3           # Traversal depth for context
graph_weight = 0.5        # Balance between graph and semantic (0.0-1.0)
graph_threshold = 0.7     # Tool recommendation threshold

# ========== MULTI-MODEL REASONING (DEFAULT: ENABLED) ==========
fast_reasoning = true      # Use fast model for simple tasks

# Fast model configuration (Llama-3.2-3B)
fast_model_provider = "mlx"  # Use MLX on Apple Silicon
fast_model_name = "mlx-community/Llama-3.2-3B-Instruct-4bit"
fast_model_temperature = 0.3  # Lower for consistency

# For cross-platform (Ollama), uncomment:
# fast_model_provider = "ollama"
# fast_model_name = "llama3.2:3b"

# Tasks delegated to fast model for 10-15x speedup
fast_model_tasks = [
    "entity_extraction",      # Extract names, dates, URLs
    "graph_analysis",         # Analyze graph relationships
    "decision_routing",       # Determine task complexity
    "tool_selection",        # Choose appropriate tools
    "confidence_scoring",    # Assess response confidence
]

# Escalate to main model if confidence < 60%
escalation_threshold = 0.6

# Display reasoning summary to user (requires fast_reasoning = true)
# Shows a concise summary of the model's thought process
show_reasoning = false  # Default: false

# ========== SPECIALIZED AGENTS ==========

[agents.coder]
# Coding assistant with syntax checking via fast model
prompt = "You are a helpful coding assistant. You write clean, well-documented code and follow best practices."
style = "professional"
temperature = 0.3
allowed_tools = ["file_read", "file_write", "bash", "search", "file_extract"]
memory_k = 25

# Use graph for code structure tracking
enable_graph = true
graph_memory = true
auto_graph = true

# Fast model for syntax and analysis
fast_reasoning = true
fast_model_temperature = 0.1  # Very low for deterministic checking
fast_model_tasks = [
    "syntax_checking",
    "linting",
    "import_resolution",
    "variable_tracking",
]

[agents.researcher]
# Research assistant with enhanced graph traversal
prompt = "You are a research assistant. You help gather, analyze, and synthesize information from various sources."
temperature = 0.5
denied_tools = ["bash", "file_write"]
memory_k = 30

# Deep graph for research connections
enable_graph = true
graph_memory = true
graph_depth = 5  # Deeper traversal
graph_weight = 0.7  # Favor graph relationships

# Fast model for document processing
fast_reasoning = true
fast_model_tasks = [
    "entity_extraction",
    "keyword_extraction",
    "metadata_extraction",
]

[agents.creative]
# Creative writing with minimal fast model usage
prompt = "You are a creative writing assistant. You help with storytelling, brainstorming, and creative expression."
temperature = 1.2
style = "casual"
memory_k = 15

# Graph for narrative tracking
enable_graph = true
graph_memory = true
graph_steering = false  # Less steering for creativity

# Minimal fast model (only for tracking)
fast_reasoning = true
fast_model_tasks = ["entity_extraction"]
escalation_threshold = 0.3  # Quickly escalate creative tasks

[agents.simple]
# Simple agent without advanced features (backward compatibility)
prompt = "You are a helpful assistant."
temperature = 0.7
memory_k = 10

# Disable advanced features
enable_graph = false
fast_reasoning = false

[agents.local_mlx]
# Pure local agent using only Llama-3.2-3B
prompt = "You are a helpful assistant running locally on Apple Silicon."
model_provider = "mlx"
model_name = "mlx-community/Llama-3.2-3B-Instruct-4bit"
temperature = 0.7
memory_k = 10

# No hierarchical reasoning (single model)
fast_reasoning = false
enable_graph = true  # Can still use graph

# ========== PERFORMANCE NOTES ==========
#
# Multi-Model Performance (with Llama-3.2-3B):
# - Entity extraction: 10-15x faster
# - Classification: 8-12x faster
# - Syntax checking: 10-15x faster
# - Cost savings: 60-80% for mixed workloads
#
# Hardware Requirements:
# - Llama-3.2-3B (4-bit): ~2GB RAM
# - M1/M2/M3 Macs: 50-120 tokens/second
#
# Installation:
# - MLX (Apple Silicon): pip install mlx mlx-lm
# - Ollama (Cross-platform): https://ollama.ai
#
# Environment Variables (override config):
# - AGENT_DB_PATH: Database path
# - AGENT_MODEL_PROVIDER: Model provider
# - AGENT_MODEL_NAME: Model name
# - AGENT_MODEL_TEMPERATURE: Temperature
# - AGENT_API_KEY_SOURCE: API key source
# - AGENT_LOG_LEVEL: Log level
# - AGENT_DEFAULT_AGENT: Default agent
# - MLX_ENDPOINT: MLX server endpoint
